<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sachindots.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://sachindots.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-21T01:32:10+00:00</updated><id>https://sachindots.github.io//feed.xml</id><title type="html">Sachin Dot</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://sachindots.github.io//blog/2024/2023-12-08-KNN/" rel="alternate" type="text/html" title=""/><published>2024-11-21T01:32:10+00:00</published><updated>2024-11-21T01:32:10+00:00</updated><id>https://sachindots.github.io//blog/2024/2023-12-08-KNN</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/2023-12-08-KNN/"><![CDATA[<p>The K-Nearest Neighbors (KNN) algorithm is a powerful and intuitive machine learning method used for classification and regression tasks. This blog post explains KNN in detail, demonstrates its application to a dataset for predicting file legitimacy, and interprets the results step by step.</p> <hr/> <h3 id="what-is-k-nearest-neighbors-knn"><strong>What is K-Nearest Neighbors (KNN)?</strong></h3> <p>KNN is a supervised machine learning algorithm used to classify data based on the characteristics of its nearest neighbors. Key aspects of KNN include:</p> <ol> <li><strong>No Assumptions</strong>: KNN does not make assumptions about the data’s underlying distribution.</li> <li><strong>Instance-Based Learning</strong>: It memorizes the training dataset to classify new instances.</li> <li><strong>Distance-Based</strong>: Predictions are made based on the distance between the test point and its nearest training data points.</li> <li><strong>Key Parameters</strong>: <ul> <li><strong>k</strong>: The number of neighbors to consider.</li> <li><strong>Distance Metric</strong>: Determines how “closeness” is calculated (e.g., Euclidean, Manhattan).</li> </ul> </li> </ol> <hr/> <h4 id="about-the-dataset"><strong>About the Dataset</strong></h4> <p>The dataset consists of features extracted from files to predict their legitimacy:</p> <ul> <li><strong>Features</strong>: Include technical attributes like <code class="language-plaintext highlighter-rouge">SizeOfOptionalHeader</code>, <code class="language-plaintext highlighter-rouge">MajorLinkerVersion</code>, <code class="language-plaintext highlighter-rouge">SizeOfCode</code>, and resource entropy.</li> <li><strong>Target Variable</strong>: <code class="language-plaintext highlighter-rouge">legitimate</code> (1 = Legitimate, 0 = Malicious).</li> <li><strong>Preprocessing</strong>: Dropping irrelevant columns (<code class="language-plaintext highlighter-rouge">Name</code>, <code class="language-plaintext highlighter-rouge">md5</code>) and standardizing features to ensure equal weighting in distance calculations.</li> </ul> <p>Kaggle Link: https://www.kaggle.com/datasets/divg07/malware-analysis-dataset</p> <hr/> <h3 id="implementation"><strong>Implementation</strong></h3> <p>Below is the Python implementation segmented into logical parts with explanations.</p> <h4 id="load-and-preprocess-the-dataset"><strong>Load and Preprocess the Dataset</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load dataset
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">'</span><span class="s">|</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Separate features and target variable
</span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">md5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">legitimate</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">legitimate</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># Split the data
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p>The columns <code class="language-plaintext highlighter-rouge">Name</code> and <code class="language-plaintext highlighter-rouge">md5</code> are identifiers and do not influence the classification. These are dropped to focus on technical attributes. Features (<code class="language-plaintext highlighter-rouge">X</code>) and target (<code class="language-plaintext highlighter-rouge">y</code>) are separated for modeling.</p> <hr/> <h4 id="standardize-the-features"><strong>Standardize the Features</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Feature scaling
</span><span class="n">sc</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Standardization ensures that features have a mean of 0 and a standard deviation of 1, preventing large-scale features from dominating the distance calculation.</li> </ul> <hr/> <h4 id="determine-the-optimal-number-of-neighbors-k"><strong>Determine the Optimal Number of Neighbors (k)</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">k_values</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_values</span><span class="p">:</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">minkowski</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="sh">'</span><span class="s">distance</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">accuracies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Plot accuracy vs. k
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy vs. Number of Neighbors (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Neighbors (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">k_values</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">best_k</span> <span class="o">=</span> <span class="n">k_values</span><span class="p">[</span><span class="n">accuracies</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">accuracies</span><span class="p">))]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The best number of neighbors (k) is: </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s"> with an accuracy of </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h5 id="output">Output</h5> <p>![[/assets/img/KNN_Graph.png]]</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The best number of neighbors (k) is: 9 with an accuracy of 0.97531
</code></pre></div></div> <p>It is essential for identifying the optimal number of neighbors (k) in the K-Nearest Neighbors (KNN) algorithm, which directly impacts model accuracy. The parameter k determines how many nearby data points are considered when making predictions, influencing the balance between overfitting and underfitting. By iterating through a range of k values (1 to 10), the code evaluates model performance using accuracy as the metric.</p> <p>Each k value is tested with the given training and test data, and the results are visualized in a plot to reveal how accuracy changes with different k. This allows data scientists to make an informed decision about the best k, ensuring optimal performance.</p> <p>Additionally, the configuration of the KNN classifier in this code uses the Minkowski metric (p=2, equivalent to Euclidean distance) and weighted distances, giving more importance to closer neighbors. This approach ensures that proximity plays a critical role in prediction, which is particularly useful when data points vary in density.</p> <hr/> <h4 id="build-the-final-model"><strong>Build the Final Model</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Final model with the best k
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">best_k</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">minkowski</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="sh">'</span><span class="s">distance</span><span class="sh">'</span><span class="p">)</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict on test set
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div> <p>We will be using the best_k that we found from the previous output visual and apply it in the KNeighborsClassifier function from the library Sklearn. For building a model, we will use the train set that we split from the dataset. For testing a model, we use the test set. There is another form of set known as the validation set which is used in Neural network to validate each epoch of a neural model training. To learn more about neural network, visit the tag neural-network in the blog/ page.</p> <hr/> <h4 id="evaluate-the-model"><strong>Evaluate the Model</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># Evaluate performance
</span><span class="n">accuracy</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">class_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Final Model Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Classification Report:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">class_report</span><span class="p">)</span>
</code></pre></div></div> <h5 id="output-1">Output</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Model Accuracy: 0.9753320683111955
Confusion Matrix:
[[1360   22]
 [  30  696]]
Classification Report:
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1382
           1       0.97      0.96      0.96       726

    accuracy                           0.98      2108
   macro avg       0.97      0.97      0.97      2108
weighted avg       0.98      0.98      0.98      2108
</code></pre></div></div> <p>The <strong>Confusion Matrix</strong> gives a detailed breakdown of the our model’s predictions and helps us understand its performance. It is a function from the module metrics in Sklearn library.</p> <p>In this case, the model correctly identified 696 legitimate files (True Positives) and 1360 malicious files (True Negatives).</p> <p>However, there were 22 instances where legitimate files were incorrectly classified as malicious (False Positives), and 30 instances where malicious files were misclassified as legitimate (False Negatives). This demonstrates that while the model performs well overall, there are some instances of misclassification that could be critical depending on the application.</p> <p>The <strong>Classification Report</strong> highlights the model’s ability to distinguish between legitimate and malicious files effectively. It provides us with precision,recall for each class of prediction.</p> <p>For legitimate files, the precision is 0.98, meaning that 98% of the files predicted as legitimate are indeed legitimate. Similarly, the recall is also 0.98, indicating that the model successfully identifies 98% of all actual legitimate files.</p> <p>These high precision and recall scores reflect the model’s reliability and robustness, ensuring that it minimizes the risk of misclassifications while maintaining a strong overall performance.</p> <hr/> <h4 id="perform-cross-validation"><strong>Perform Cross-Validation</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">cv_scores</span> <span class="o">=</span> <span class="nf">cross_val_score</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross-Validation Scores: </span><span class="si">{</span><span class="n">cv_scores</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Accuracy: </span><span class="si">{</span><span class="n">cv_scores</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Standard Deviation: </span><span class="si">{</span><span class="n">cv_scores</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Cross validation is the process by which we change the split of the dataset into n numbers and divide them into train and test to check the consistency of our model across the dataset. Finally, In this dataset, CV was performed to ensure the model’s stability and generalizability. The mean cross-validation accuracy was 0.9694, with a low standard deviation of 0.0055, demonstrating that the model’s performance is consistent across different subsets of the data.</p> <hr/> <p>KNN proved to be a highly effective classifier for predicting file legitimacy in this dataset. Its simplicity and adaptability make it a valuable tool for binary classification problems, provided the data is preprocessed and the right <code class="language-plaintext highlighter-rouge">k</code> is chosen.</p>]]></content><author><name></name></author></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Create Your Own Neural Network from Scratch</title><link href="https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork/" rel="alternate" type="text/html" title="Create Your Own Neural Network from Scratch"/><published>2024-02-14T10:00:00+00:00</published><updated>2024-02-14T10:00:00+00:00</updated><id>https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork/"><![CDATA[<p>In this blog post, we’ll explore how to create a simple neural network from scratch using Python. We’ll walk through building a Single Layer Perceptron to classify diabetes patients based on a dataset from Kaggle. You don’t need any fancy deep learning libraries like TensorFlow or PyTorch; we’ll implement everything from scratch. By the end of this blog, you’ll understand the core concepts behind neural networks and have a functional perceptron model. Let’s get started!</p> <hr/> <h3 id="what-is-a-single-layer-perceptron">What is a Single Layer Perceptron?</h3> <p>A <strong>Single Layer Perceptron (SLP)</strong> is the simplest type of artificial neural network. It consists of a single layer of weights that connect the input features to the output node, with an activation function applied to produce the final output. The perceptron was originally introduced by Frank Rosenblatt in 1958 as a binary classifier—its job is to determine whether an input belongs to one of two classes.</p> <h3 id="how-does-it-work">How Does It Work?</h3> <p>In a Single Layer Perceptron:</p> <ol> <li><strong>Inputs (Features)</strong>: The perceptron takes multiple input features (e.g., attributes from the dataset).</li> <li><strong>Weights</strong>: Each input is associated with a weight that determines its contribution to the output. These weights are initially set randomly and updated during the training process.</li> <li><strong>Weighted Sum Calculation</strong>: The perceptron computes the weighted sum of the inputs and biases.</li> <li><strong>Activation Function</strong>: The weighted sum is then passed through an activation function that maps the output to a desired range, usually between 0 and 1 for binary classification.</li> <li><strong>Output</strong>: The activation function’s output is used to make the final prediction.</li> </ol> <h3 id="understanding-a-single-layer-perceptron">Understanding a Single Layer Perceptron</h3> <p>In this diagram:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       Input Layer                      Output
   +-------------+                 +------------+
   |             |   Weights       |            |
   |   (X1)  ● ---- W1 ----+       |     ●      |
   |             |         |       |   Output   |
   |   (X2)  ● ---- W2 ----|-----&gt; |     (Y)    |
   |             |         |       |            |
   |   (X3)  ● ---- W3 ----+       +------------+
   |             |
   +-------------+
</code></pre></div></div> <ul> <li>Each input feature (<code class="language-plaintext highlighter-rouge">X1</code>, <code class="language-plaintext highlighter-rouge">X2</code>, <code class="language-plaintext highlighter-rouge">X3</code>) is represented as a circle in the input layer.</li> <li>These inputs are connected to the output neuron through weighted connections (<code class="language-plaintext highlighter-rouge">W1</code>, <code class="language-plaintext highlighter-rouge">W2</code>, <code class="language-plaintext highlighter-rouge">W3</code>).</li> <li>The output neuron computes a weighted sum of the inputs and applies an activation function to produce the output (<code class="language-plaintext highlighter-rouge">Y</code>).</li> </ul> <hr/> <h3 id="implementing-the-custom-perceptron-network">Implementing the Custom Perceptron Network</h3> <h4 id="setting-up-the-diabetes-dataset">Setting Up the Diabetes Dataset</h4> <p>To train our perceptron, we’ll use the <strong>Diabetes Dataset</strong> from Kaggle. This dataset contains several medical attributes such as glucose levels, BMI, and insulin levels, which are used to classify whether a patient is diabetic (1) or non-diabetic (0). The target variable is outcome which is in binary format (0 &amp; 1)</p> <p>Link: <a href="https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data">Diabetes Dataset</a></p> <h3 id="data-preparation">Data Preparation</h3> <p>First, let’s load and prepare the data:</p> <p><strong>Normalize the Features</strong>: It’s important to scale the input features so that the model can learn effectively. Each feature is in a different measure so we will be using StandardScaler to normalize the values.</p> <p>There are 8 Features in the dataset</p> <ul> <li>Pregnancies: Number of times pregnant</li> <li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li> <li>BloodPressure: Diastolic blood pressure (mm Hg)</li> <li>SkinThickness: Triceps skin fold thickness (mm)</li> <li>Insulin: 2-Hour serum insulin (mu U/ml)</li> <li>BMI: Body mass index (weight in kg/(height in m)^2)</li> <li>DiabetesPedigreeFunction: Diabetes pedigree function</li> <li>Age: Age (years)</li> </ul> <p><strong>Importing the data and Splitting it into Train and Test sets</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
data = pd.read_csv('diabetes.csv')

X = data.iloc[:, :-1].values
Y = data.iloc[:, -1].values # The last feature is the target variable "Outcome"

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

</code></pre></div></div> <p><strong>Applying Normalization using Standard Scaler from sklearn</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
</code></pre></div></div> <h3 id="building-the-perceptron-from-scratch">Building the Perceptron from Scratch</h3> <p>In this section, we will walk through the implementation of a Single Layer Perceptron (SLP) using Python. We’ll break down the key components of the code and explain how each part contributes to the training and prediction process. We initialize a function called train_neural_net and this willl have four parameters X,Y,learning_rate,epochs. X - the features Y- the target variable learning rate - This determines how much the weights are updated at each step. Epoch - Iterating on the each input and modifying the weights each time.</p> <h4 id="initializing-the-weights">Initializing the Weights</h4> <p>We will generate weights for each feature in X_train using np.random.rand function and converting the weights to a numerical list (since the np library generates all values in numpy format)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights = np.random.rand(X_train.shape[1])
w = weights.tolist() 
</code></pre></div></div> <h4 id="running-each-input-into-the-network">Running each input into the Network</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for epoch in range(epochs):
        ypred = []
        for i in range(len(X)):
            y = sum(X[i][j] * w[j] for j in range(len(w)))
            output = 1 if y &gt; 0.5 else 0
            ypred.append(output)
</code></pre></div></div> <h4 id="writing-a-simple-binary-activation-function">Writing a simple Binary Activation function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>			output = 1 if y &gt; 0.5 else 0
</code></pre></div></div> <h4 id="updating-the-error-using-gradient-descent">Updating the error using Gradient Descent</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>			error = Y[i] - output

			for j in range(len(w)):
                w[j] += learning_rate * error * X[i][j]
</code></pre></div></div> <h4 id="final-neural-network-function">Final Neural Network function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def train_neural_net(X, Y, learning_rate=0.01, epochs=100):
    # Initialize Weights randomly
    weights = np.random.rand(X.shape[1])
    w = weights.tolist()
    for epoch in range(epochs):
        ypred = []
        for i in range(len(X)):
            y = sum(X[i][j] * w[j] for j in range(len(w)))
            # Activation Function is applied in the output variable
            output = 1 if y &gt; 0.2 else 0
            ypred.append(output)
            error = Y[i] - output
            # Updating the error using Gradient Descent
            for j in range(len(w)):
                w[j] += learning_rate * error * X[i][j]
    return w
</code></pre></div></div> <h4 id="prediction-function">Prediction function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def predict(X, w):
    ypred = []
    for i in range(len(X)):
        y = sum(X[i][j] * w[j] for j in range(len(w)))
        # Apply the binary activation function
        output = 1 if y &gt; 0.2 else 0  # Using a threshold of 0.5
        ypred.append(output)
    return ypred
</code></pre></div></div> <h4 id="calling-the-function-and-applying-it-to-our-dataset">Calling the function and Applying it to our dataset</h4> <p>We will now send the test set to the predict function with the weights from our train_neural_net</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>final_weights = train_neural_net(X_train, Y_train)
ypred=predict(X_test,final_weights)
</code></pre></div></div> <h4 id="evaluating-the-model-performance">Evaluating the Model Performance</h4> <p>We will be using the metrics module from sklearn to implement the evaluation of our model using accuracy_score, confusion_matrix and classification_report. This helps us to understand How our model precisely identifies the diabetic patients</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

accuracy = accuracy_score(Y_test,ypred)
conf_matrix = confusion_matrix(Y_test, ypred)
class_report = classification_report(Y_test, ypred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)
</code></pre></div></div> <h4 id="results">Results</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 76.62%

Confusion Matrix:
[[89 10]
 [26 29]]

Classification Report:
              precision    recall  f1-score   support

           0       0.77      0.90      0.83        99
           1       0.74      0.53      0.62        55

    accuracy                           0.77       154
   macro avg       0.76      0.71      0.72       154
weighted avg       0.76      0.77      0.76       154
</code></pre></div></div> <p>The model achieved an accuracy of 76.62%, indicating it performed reasonably well in predicting diabetes but showed some limitations, especially in identifying diabetic cases (Class 1).</p> <p>The precision and recall for Class 1 suggest that while the model identified many true positives, it missed a significant portion of them, leading to a lower recall.</p> <p>To improve the model, we could try techniques like hyperparameter tuning, increasing the number of neurons or layers in the network, using a balanced dataset with techniques like SMOTE to address class imbalance, and experimenting with advanced optimizers or regularization methods to enhance generalization.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="neural-network"/><summary type="html"><![CDATA[How to make your own neural network using Python from scratch]]></summary></entry><entry><title type="html">CEH Basics - Information Security - Terms and Concepts</title><link href="https://sachindots.github.io//blog/2024/CEH_Basics-1/" rel="alternate" type="text/html" title="CEH Basics - Information Security - Terms and Concepts"/><published>2024-01-04T12:57:00+00:00</published><updated>2024-01-04T12:57:00+00:00</updated><id>https://sachindots.github.io//blog/2024/CEH_Basics-1</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/CEH_Basics-1/"><![CDATA[<p>In today’s AI era, the importance of securing information has never been greater. Whether you’re a cybersecurity enthusiast or a professional, understanding the basics of information security is crucial for navigating this dynamic field. This blog breaks down the core principles, frameworks, and emerging trends in information security to help you build a solid foundation to help you in your path to a Certified Ethical Hacker.</p> <hr/> <h3 id="what-is-information-security"><strong>What is Information Security?</strong></h3> <p>Information Security (InfoSec) refers to safeguarding sensitive information from unauthorized access, misuse, or destruction. It ensures the <strong>Confidentiality, Integrity, Availability</strong>, and <strong>Authenticity</strong> of data while preventing unauthorized activities.</p> <hr/> <h3 id="core-elements-of-information-security"><strong>Core Elements of Information Security</strong></h3> <ol> <li><strong>Confidentiality</strong>: Ensures that sensitive information is accessible only to authorized personnel.</li> <li><strong>Integrity</strong>: Maintains trustworthiness by ensuring data is not altered by unauthorized entities (e.g., through techniques like hashing).</li> <li><strong>Availability</strong>: Guarantees that data and resources are accessible to users whenever needed.</li> <li><strong>Authenticity</strong>: Validates the identity of users and systems accessing resources, ensuring data genuineness.</li> <li><strong>Non-Repudiation</strong>: Prevents denial of actions, ensuring that senders and recipients cannot deny sending or receiving messages.</li> </ol> <hr/> <h3 id="understanding-attacks-motives-goals-and-objectives"><strong>Understanding Attacks: Motives, Goals, and Objectives</strong></h3> <p>Cyberattacks arise from various motives such as financial gain, data theft, or resource exploitation. Here’s how these elements interplay:</p> <ul> <li><strong>Motive</strong>: The attacker’s reason for targeting specific systems (e.g., money, intellectual property).</li> <li><strong>Goal</strong>: Achieving unauthorized access or disrupting services.</li> <li><strong>Method (TTP)</strong>: The tactics, techniques, and procedures used to exploit vulnerabilities.</li> </ul> <p>Attacks = Motive (Goal) * Method (TTP) * Vulnerability</p> <h4 id="vulnerabilities">Vulnerabilities</h4> <ul> <li>The weakness or technical malfunction of a software or a hardware which can lead a hacker to use it for his benefit to extract data from a system.</li> </ul> <h4 id="tactics-techniques-and-procedures-ttp">Tactics, Techniques, and Procedures (TTP)</h4> <ul> <li><strong>Tactics</strong>: High-level strategies that outline an attack from start to finish.</li> <li><strong>Techniques</strong>: The specific methods used to achieve intermediate goals (e.g., phishing, SQL injection).</li> <li><strong>Procedures</strong>: Step-by-step execution plans for attacks.</li> </ul> <hr/> <h3 id="types-of-attacks"><strong>Types of Attacks</strong></h3> <ol> <li><strong>Passive Attacks</strong>: Eavesdropping, network sniffing, or OSINT methods to gather data without altering it.</li> <li><strong>Active Attacks</strong>: Tampering with data or breaking security protocols (e.g., ransomware).</li> <li><strong>Close-In Attacks</strong>: Physical proximity-based attacks, such as stealing hardware or tampering with devices.</li> <li><strong>Insider Attacks</strong>: Exploiting insider knowledge or planting malicious insiders.</li> <li><strong>Distribution Attacks</strong>: Tampering with software or hardware during the supply chain process.</li> </ol> <hr/> <h3 id="ethical-hacking-think-like-a-hacker"><strong>Ethical Hacking: Think Like a Hacker</strong></h3> <p>To combat cybercriminals, ethical hacking focuses on anticipating malicious tactics. Ethical hackers simulate attacks to identify and mitigate vulnerabilities.</p> <h4 id="skills-of-an-ethical-hacker"><strong>Skills of an Ethical Hacker</strong></h4> <p><strong>Technical Skills</strong>:</p> <ul> <li>Knowledge of operating systems (Windows, Linux, macOS)</li> <li>Networking fundamentals</li> <li>Expertise in security tools and methods</li> </ul> <p><strong>Non-Technical Skills</strong>:</p> <ul> <li>Strong analytical, communication, and problem-solving abilities</li> <li>Adherence to organizational security policies and ethical standards</li> </ul> <hr/> <h3 id="modern-trends-ai-driven-hacking"><strong>Modern Trends: AI-Driven Hacking</strong></h3> <p>Artificial Intelligence (AI) is revolutionizing cybersecurity, enabling automated vulnerability detection and response. It involves using AI Algorithms, ML Models, automation frameworks to faciliate and automate ethical hacking efforts. However, it also introduces new risks:</p> <ul> <li><strong>Sophisticated Phishing</strong>: AI can generate convincing fake emails.</li> <li><strong>Automated Exploit Generation</strong>: Leveraging machine learning to exploit zero-day vulnerabilities.</li> <li>It helps hackers by automatic routine tasks in assistance.</li> <li><strong>Accelerated LLM Analysis</strong> : LLMs can rapidly analyze massive datasets, including security logs and network traffic, to identify patterns that may indicate a breach or vulnerability.</li> <li><strong>Scripting and Coding</strong> : Writing backdoors, Exploits for a given scenario improves hacker’s work easy.</li> </ul> <p>ShellGPT , a Command line based GPT, is one such tool to support terminal based LLM Assistance for ethical hackers and programmers</p> <hr/> <h3 id="frameworks-used-in-information-security">Frameworks used in Information Security</h3> <p>Frameworks in information security provide structured methodologies to manage risks, mitigate threats, and establish robust cybersecurity practices. They offer guidelines for organizations to systematically secure their assets, ensure compliance, and effectively respond to incidents. Here, we explore key frameworks that form the backbone of modern cybersecurity strategies.</p> <h4 id="1-ceh-ethical-hacking-framework">1. <strong>CEH Ethical Hacking Framework</strong></h4> <p>This framework outlines a systematic approach for ethical hackers to test and identify vulnerabilities in a system. The steps include:</p> <p><strong>Phase 1: Reconnaissance</strong></p> <ul> <li><strong>Footprinting and Reconnaissance</strong>: Gathering data about the target through open-source intelligence (OSINT) and passive information gathering.</li> <li><strong>Scanning and Enumeration</strong>: Identifying live hosts, open ports, and services through tools like Nmap and Wireshark.</li> </ul> <p><strong>Phase 2: Vulnerability Scanning</strong></p> <ul> <li><strong>Vulnerability Analysis</strong>: Scanning for misconfigurations, outdated software, and exploitable weaknesses using tools like Nessus and OpenVAS.</li> </ul> <p><strong>Phase 3: Gaining Access</strong></p> <ul> <li>Exploiting vulnerabilities to infiltrate systems. Techniques may include exploiting misconfigured applications or privilege escalation.</li> </ul> <p><strong>Phase 4: Maintaining Access</strong></p> <ul> <li>Creating backdoors and persistent threats to ensure continued control over the system for further investigation.</li> </ul> <p><strong>Phase 5: Clearing Tracks</strong></p> <ul> <li>Deleting logs, hiding scripts, or using anti-forensics techniques to ensure the attack is untraceable.</li> </ul> <h4 id="2-cyber-kill-chain-methodology">2. <strong>Cyber Kill Chain Methodology</strong></h4> <p>The <strong>Cyber Kill Chain</strong>, developed by Lockheed Martin, maps out the stages of a cyberattack and provides a framework to disrupt adversary operations.</p> <ol> <li><strong>Reconnaissance</strong>: Researching the target to gather exploitable information.</li> <li><strong>Weaponization</strong>: Developing malware, exploits, or phishing kits to target specific vulnerabilities.</li> <li><strong>Delivery</strong>: Transmitting the malicious payload via phishing emails, USB devices, or infected websites.</li> <li><strong>Exploitation</strong>: Triggering the payload by exploiting system vulnerabilities.</li> <li><strong>Installation</strong>: Placing malware or backdoors for continuous access.</li> <li><strong>Command and Control (C2)</strong>: Establishing communication with the attacker’s server.</li> <li><strong>Actions on Objectives</strong>: Extracting data, disrupting services, or achieving the attacker’s goals.</li> </ol> <h4 id="3-mitre-attck-framework">3. <strong>MITRE ATT&amp;CK Framework</strong></h4> <p>The <strong>MITRE ATT&amp;CK</strong> (Adversarial Tactics, Techniques, and Common Knowledge) framework is a globally recognized knowledge base detailing the behavior of adversaries. It is a framework globally accessible knowledge base of adversary tactics and techniques based on real-world observations</p> <p>The ATT&amp;CK Knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sectorm government and the cybersecurity product and service community.</p> <p>The 14 tactic categories within ATT&amp;CK for Enterprise are derived from the later stages of the sevens stages of Cyberkillchain</p> <p>Recon -&gt; Weaponize -&gt; Deliver -&gt; Exploit -&gt; Control -&gt; Execute -&gt; Maintain { PRE - ATT&amp;CK } { Enterprise ATT&amp;CK }</p> <h4 id="4-diamond-model-of-intrusion-analysis">4. <strong>Diamond Model of Intrusion Analysis</strong></h4> <p>This model emphasizes understanding cyber intrusions by examining their key elements:</p> <ul> <li><strong>Adversary</strong>: The attacker or threat actor.</li> <li><strong>Victim</strong>: The system or organization being targeted.</li> <li><strong>Capability</strong>: Tools, tactics, or techniques used in the attack.</li> <li><strong>Infrastructure</strong>: Assets like servers, IPs, or domains leveraged by the attacker.</li> </ul> <p>The Diamond Model helps in correlating attack data, understanding adversarial behavior, and predicting future actions.</p> <h4 id="5-defense-in-depth-framework">5. <strong>Defense-in-Depth Framework</strong></h4> <p>A <strong>Defense-in-Depth</strong> approach integrates multiple layers of security to protect assets:</p> <ol> <li><strong>Policies and Awareness</strong>: Security training and compliance policies.</li> <li><strong>Physical Security</strong>: Securing access to premises and hardware.</li> <li><strong>Perimeter Security</strong>: Firewalls and intrusion detection systems.</li> <li><strong>Network Security</strong>: Segmentation and secure configurations.</li> <li><strong>Host Security</strong>: Protecting individual devices with endpoint security solutions.</li> <li><strong>Application Security</strong>: Secure coding practices and penetration testing.</li> <li><strong>Data Security</strong>: Encryption, backup, and access control.</li> </ol> <p>This layered approach ensures that breaching one layer does not compromise the entire system.</p> <hr/> <h3 id="regulations-in-information-security">Regulations in Information Security</h3> <p>Regulatory frameworks mandate organizations to adhere to specific security and privacy standards, ensuring data protection and minimizing risks. Here’s a detailed look at some pivotal regulations:</p> <h4 id="1-general-data-protection-regulation-gdpr"><strong>1. General Data Protection Regulation (GDPR)</strong></h4> <ul> <li><strong>Jurisdiction</strong>: European Union (EU), but applies globally to organizations handling EU citizens’ data.</li> <li><strong>Purpose</strong>: Grants individuals rights over their personal data, including the right to access, correct, and delete their information.</li> <li><strong>Key Features</strong>: <ul> <li>Mandatory data breach notification within 72 hours.</li> <li>Heavy penalties for non-compliance (up to €20 million or 4% of annual global turnover).</li> <li>Requires Data Protection Officers (DPOs) for organizations processing large-scale personal data.</li> </ul> </li> </ul> <h4 id="2-payment-card-industry-data-security-standard-pci-dss"><strong>2. Payment Card Industry Data Security Standard (PCI DSS)</strong></h4> <ul> <li><strong>Scope</strong>: Entities involved in payment card processing.</li> <li><strong>Key Requirements</strong>: <ul> <li>Encrypt cardholder data transmission.</li> <li>Regularly update and patch systems.</li> <li>Implement access control measures.</li> </ul> </li> <li><strong>Benefits</strong>: Protects against card fraud and ensures secure transactions.</li> </ul> <h4 id="3-isoiec-standards"><strong>3. ISO/IEC Standards</strong></h4> <ul> <li><strong>ISO/IEC 27001:2022</strong>: Establishes an Information Security Management System (ISMS).</li> <li><strong>ISO/IEC 27701:2019</strong>: Extends ISMS to include Privacy Information Management (PIMS).</li> <li><strong>ISO/IEC 27002:2022</strong>: Best practices for implementing controls.</li> <li><strong>ISO/IEC 27036-3:2023</strong>: Secures supply chains for hardware, software, and services.</li> <li><strong>ISO/IEC 27040:2024</strong>: Guidelines for secure data storage.</li> </ul> <h4 id="4-health-insurance-portability-and-accountability-act-hipaa"><strong>4. Health Insurance Portability and Accountability Act (HIPAA)</strong></h4> <ul> <li><strong>Scope</strong>: Protects patient data in the healthcare industry.</li> <li><strong>Requirements</strong>: <ul> <li>Secure electronic health records (EHRs).</li> <li>Regular audits and risk assessments.</li> <li>Training employees on data protection.</li> </ul> </li> </ul> <h4 id="5-sarbanes-oxley-act-sox"><strong>5. Sarbanes-Oxley Act (SOX)</strong></h4> <ul> <li><strong>Purpose</strong>: Protects investors by ensuring accurate financial disclosures.</li> <li><strong>Cybersecurity Relevance</strong>: Emphasizes secure handling and transmission of financial data.</li> </ul> <h4 id="6-digital-millennium-copyright-act-dmca"><strong>6. Digital Millennium Copyright Act (DMCA)</strong></h4> <ul> <li><strong>Focus</strong>: Protects intellectual property online.</li> <li><strong>Cybersecurity Impact</strong>: Prevents unauthorized distribution of copyrighted digital content.</li> </ul> <h4 id="7-federal-information-security-management-act-fisma"><strong>7. Federal Information Security Management Act (FISMA)</strong></h4> <ul> <li><strong>Scope</strong>: U.S. federal agencies and contractors.</li> <li><strong>Requirements</strong>: <ul> <li>Conduct annual security reviews.</li> <li>Implement risk management frameworks.</li> </ul> </li> </ul> <h4 id="8-information-technology-it-act-2000-india"><strong>8. Information Technology (IT) Act, 2000 (India)</strong></h4> <ul> <li><strong>Objective</strong>: Regulates e-commerce and cybercrime.</li> <li><strong>Provisions</strong>: <ul> <li>Defines electronic signatures.</li> <li>Penalizes hacking, identity theft, and other cybercrimes.</li> </ul> </li> </ul> <hr/> <p>Information security is a dynamic field requiring constant vigilance, adaptability, and innovation. Whether you’re a beginner or a seasoned professional, understanding the fundamental concepts and frameworks is key to staying ahead in this ever-evolving domain. By combining traditional approaches with AI-driven advancements, we can build a resilient digital ecosystem that safeguards critical data and systems.</p> <p>Hack, Learn, Repeat!</p>]]></content><author><name></name></author><category term="cyber-security"/><category term="cybersecurity"/><category term="CEH"/><summary type="html"><![CDATA[Learn about various concepts in Information Security]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with images</title><link href="https://sachindots.github.io//blog/2015/images/" rel="alternate" type="text/html" title="a post with images"/><published>2015-05-15T21:01:00+00:00</published><updated>2015-05-15T21:01:00+00:00</updated><id>https://sachindots.github.io//blog/2015/images</id><content type="html" xml:base="https://sachindots.github.io//blog/2015/images/"><![CDATA[<p>This is an example post with image galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9.jpg" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7.jpg" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8.jpg" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10.jpg" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/11.jpg" sizes="95vw"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12.jpg" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7.jpg" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">a post with formatting and links</title><link href="https://sachindots.github.io//blog/2015/formatting-and-links/" rel="alternate" type="text/html" title="a post with formatting and links"/><published>2015-03-15T16:40:16+00:00</published><updated>2015-03-15T16:40:16+00:00</updated><id>https://sachindots.github.io//blog/2015/formatting-and-links</id><content type="html" xml:base="https://sachindots.github.io//blog/2015/formatting-and-links/"><![CDATA[<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <h4 id="check-list">Check List</h4> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Brush Teeth</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on socks <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Put on left sock</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on right sock</li> </ul> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Go to school</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90’s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> <p>We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin</p> </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>