<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sachindots.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://sachindots.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-20T16:23:13+00:00</updated><id>https://sachindots.github.io//feed.xml</id><title type="html">Sachin Dot</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://sachindots.github.io//blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://sachindots.github.io//blog/2024/pseudocode</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">Create Your Own Neural Network from Scratch</title><link href="https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork/" rel="alternate" type="text/html" title="Create Your Own Neural Network from Scratch"/><published>2024-02-14T10:00:00+00:00</published><updated>2024-02-14T10:00:00+00:00</updated><id>https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork</id><content type="html" xml:base="https://sachindots.github.io//blog/2024/CreateYourOwnNeuralNetwork/"><![CDATA[<h1 id="create-your-own-neural-network-for-dummies">Create Your Own Neural Network for Dummies</h1> <p>In this blog post, we’ll explore how to create a simple neural network from scratch using Python. We’ll walk through building a Single Layer Perceptron to classify diabetes patients based on a dataset from Kaggle. You don’t need any fancy deep learning libraries like TensorFlow or PyTorch; we’ll implement everything from scratch. By the end of this blog, you’ll understand the core concepts behind neural networks and have a functional perceptron model. Let’s get started!</p> <hr/> <h2 id="what-is-a-single-layer-perceptron">What is a Single Layer Perceptron?</h2> <p>A <strong>Single Layer Perceptron (SLP)</strong> is the simplest type of artificial neural network. It consists of a single layer of weights that connect the input features to the output node, with an activation function applied to produce the final output. The perceptron was originally introduced by Frank Rosenblatt in 1958 as a binary classifier—its job is to determine whether an input belongs to one of two classes.</p> <h3 id="how-does-it-work">How Does It Work?</h3> <p>In a Single Layer Perceptron:</p> <ol> <li><strong>Inputs (Features)</strong>: The perceptron takes multiple input features (e.g., attributes from the dataset).</li> <li><strong>Weights</strong>: Each input is associated with a weight that determines its contribution to the output. These weights are initially set randomly and updated during the training process.</li> <li><strong>Weighted Sum Calculation</strong>: The perceptron computes the weighted sum of the inputs and biases.</li> <li><strong>Activation Function</strong>: The weighted sum is then passed through an activation function that maps the output to a desired range, usually between 0 and 1 for binary classification.</li> <li><strong>Output</strong>: The activation function’s output is used to make the final prediction. <h3 id="understanding-a-single-layer-perceptron">Understanding a Single Layer Perceptron</h3> </li> </ol> <p>In this diagram:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       Input Layer                      Output
   +-------------+                 +------------+
   |             |   Weights       |            |
   |   (X1)  ● ---- W1 ----+       |     ●      |
   |             |         |       |   Output   |
   |   (X2)  ● ---- W2 ----|-----&gt; |     (Y)    |
   |             |         |       |            |
   |   (X3)  ● ---- W3 ----+       +------------+
   |             |
   +-------------+
</code></pre></div></div> <ul> <li>Each input feature (<code class="language-plaintext highlighter-rouge">X1</code>, <code class="language-plaintext highlighter-rouge">X2</code>, <code class="language-plaintext highlighter-rouge">X3</code>) is represented as a circle in the input layer.</li> <li>These inputs are connected to the output neuron through weighted connections (<code class="language-plaintext highlighter-rouge">W1</code>, <code class="language-plaintext highlighter-rouge">W2</code>, <code class="language-plaintext highlighter-rouge">W3</code>).</li> <li>The output neuron computes a weighted sum of the inputs and applies an activation function to produce the output (<code class="language-plaintext highlighter-rouge">Y</code>).</li> </ul> <h3 id="implementing-the-custom-perceptron-network">Implementing the Custom Perceptron Network</h3> <h4 id="setting-up-the-diabetes-dataset">Setting Up the Diabetes Dataset</h4> <p>To train our perceptron, we’ll use the <strong>Diabetes Dataset</strong> from Kaggle. This dataset contains several medical attributes such as glucose levels, BMI, and insulin levels, which are used to classify whether a patient is diabetic (1) or non-diabetic (0). The target variable is outcome which is in binary format (0 &amp; 1)</p> <p>Link: <a href="https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data">Diabetes Dataset</a></p> <h3 id="data-preparation">Data Preparation</h3> <p>First, let’s load and prepare the data:</p> <p><strong>Normalize the Features</strong>: It’s important to scale the input features so that the model can learn effectively. Each feature is in a different measure so we will be using StandardScaler to normalize the values.</p> <p>There are 8 Features in the dataset</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Pregnancies: Number of times pregnant
- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
- BloodPressure: Diastolic blood pressure (mm Hg)
- SkinThickness: Triceps skin fold thickness (mm)
- Insulin: 2-Hour serum insulin (mu U/ml)
- BMI: Body mass index (weight in kg/(height in m)^2)
- DiabetesPedigreeFunction: Diabetes pedigree function
- Age: Age (years)
</code></pre></div></div> <p><strong>Train-Test Split</strong> : We’ll split the dataset into training and testing sets for model evaluation</p> <p><strong>Importing the data and Splitting it into Train and Test sets</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
data = pd.read_csv('diabetes.csv')

X = data.iloc[:, :-1].values
Y = data.iloc[:, -1].values # The last feature is the target variable "Outcome"

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

</code></pre></div></div> <p><strong>Applying Normalization using Standard Scaler from sklearn</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
</code></pre></div></div> <h3 id="building-the-perceptron-from-scratch">Building the Perceptron from Scratch</h3> <p>In this section, we will walk through the implementation of a Single Layer Perceptron (SLP) using Python. We’ll break down the key components of the code and explain how each part contributes to the training and prediction process. We initialize a function called train_neural_net and this willl have four parameters X,Y,learning_rate,epochs. X - the features Y- the target variable learning rate - This determines how much the weights are updated at each step. Epoch - Iterating on the each input and modifying the weights each time.</p> <h4 id="initializing-the-weights">Initializing the Weights</h4> <p>We will generate weights for each feature in X_train using np.random.rand function and converting the weights to a numerical list (since the np library generates all values in numpy format)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights = np.random.rand(X_train.shape[1])
w = weights.tolist() 
</code></pre></div></div> <h4 id="running-each-input-into-the-network">Running each input into the Network</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for epoch in range(epochs):
        ypred = []
        for i in range(len(X)):
            y = sum(X[i][j] * w[j] for j in range(len(w)))
            output = 1 if y &gt; 0.5 else 0
            ypred.append(output)
</code></pre></div></div> <h4 id="writing-a-simple-binary-activation-function">Writing a simple Binary Activation function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>			output = 1 if y &gt; 0.5 else 0
</code></pre></div></div> <h4 id="updating-the-error-using-gradient-descent">Updating the error using Gradient Descent</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>			error = Y[i] - output

			for j in range(len(w)):
                w[j] += learning_rate * error * X[i][j]
</code></pre></div></div> <h4 id="final-neural-network-function">Final Neural Network function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def train_neural_net(X, Y, learning_rate=0.01, epochs=100):
    # Initialize Weights randomly
    weights = np.random.rand(X.shape[1])
    w = weights.tolist()
    for epoch in range(epochs):
        ypred = []
        for i in range(len(X)):
            y = sum(X[i][j] * w[j] for j in range(len(w)))
            # Activation Function is applied in the output variable
            output = 1 if y &gt; 0.2 else 0
            ypred.append(output)
            error = Y[i] - output
            # Updating the error using Gradient Descent
            for j in range(len(w)):
                w[j] += learning_rate * error * X[i][j]
    return w
</code></pre></div></div> <h4 id="prediction-function">Prediction function</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def predict(X, w):
    ypred = []
    for i in range(len(X)):
        y = sum(X[i][j] * w[j] for j in range(len(w)))
        # Apply the binary activation function
        output = 1 if y &gt; 0.2 else 0  # Using a threshold of 0.5
        ypred.append(output)
    return ypred
</code></pre></div></div> <h4 id="calling-the-function-and-applying-it-to-our-dataset">Calling the function and Applying it to our dataset</h4> <p>We will now send the test set to the predict function with the weights from our train_neural_net</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>final_weights = train_neural_net(X_train, Y_train)
ypred=predict(X_test,final_weights)
</code></pre></div></div> <h4 id="evaluating-the-model-performance">Evaluating the Model Performance</h4> <p>We will be using the metrics module from sklearn to implement the evaluation of our model using accuracy_score, confusion_matrix and classification_report. This helps us to understand How our model precisely identifies the diabetic patients</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

accuracy = accuracy_score(Y_test,ypred)
conf_matrix = confusion_matrix(Y_test, ypred)
class_report = classification_report(Y_test, ypred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)
</code></pre></div></div> <h4 id="result">Result</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 76.62%

Confusion Matrix:
[[89 10]
 [26 29]]

Classification Report:
              precision    recall  f1-score   support

           0       0.77      0.90      0.83        99
           1       0.74      0.53      0.62        55

    accuracy                           0.77       154
   macro avg       0.76      0.71      0.72       154
weighted avg       0.76      0.77      0.76       154
</code></pre></div></div> <p>The model achieved an accuracy of 76.62%, indicating it performed reasonably well in predicting diabetes but showed some limitations, especially in identifying diabetic cases (Class 1).</p> <p>The precision and recall for Class 1 suggest that while the model identified many true positives, it missed a significant portion of them, leading to a lower recall.</p> <p>To improve the model, we could try techniques like hyperparameter tuning, increasing the number of neurons or layers in the network, using a balanced dataset with techniques like SMOTE to address class imbalance, and experimenting with advanced optimizers or regularization methods to enhance generalization.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[How to make your own neural network using Python from scratch]]></summary></entry><entry><title type="html">a post with jupyter notebook</title><link href="https://sachindots.github.io//blog/2023/jupyter-notebook/" rel="alternate" type="text/html" title="a post with jupyter notebook"/><published>2023-07-04T12:57:00+00:00</published><updated>2023-07-04T12:57:00+00:00</updated><id>https://sachindots.github.io//blog/2023/jupyter-notebook</id><content type="html" xml:base="https://sachindots.github.io//blog/2023/jupyter-notebook/"><![CDATA[<p>To include a jupyter notebook in a post, you can use the following code:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}
<span class="cp">{%</span><span class="w"> </span><span class="nt">assign</span><span class="w"> </span><span class="nv">jupyter_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'assets/jupyter/blog.ipynb'</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nf">relative_url</span><span class="w"> </span><span class="cp">%}</span>
<span class="cp">{%</span><span class="w"> </span><span class="nt">capture</span><span class="w"> </span><span class="nv">notebook_exists</span><span class="w"> </span><span class="cp">%}{%</span><span class="w"> </span><span class="nt">file_exists</span><span class="w"> </span>assets/jupyter/blog.ipynb<span class="w"> </span><span class="cp">%}{%</span><span class="w"> </span><span class="nt">endcapture</span><span class="w"> </span><span class="cp">%}</span>
<span class="cp">{%</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="nv">notebook_exists</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">'true'</span><span class="w"> </span><span class="cp">%}</span>
  <span class="cp">{%</span><span class="w"> </span><span class="nt">jupyter_notebook</span><span class="w"> </span><span class="nv">jupyter_path</span><span class="w"> </span><span class="cp">%}</span>
<span class="cp">{%</span><span class="w"> </span><span class="nt">else</span><span class="w"> </span><span class="cp">%}</span>
  &lt;p&gt;Sorry, the notebook you are looking for does not exist.&lt;/p&gt;
<span class="cp">{%</span><span class="w"> </span><span class="nt">endif</span><span class="w"> </span><span class="cp">%}</span>
{:/nomarkdown}
</code></pre></div></div> <p>Let’s break it down: this is possible thanks to <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll Jupyter Notebook plugin</a> that allows you to embed jupyter notebooks in your posts. It basically calls <a href="https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html"><code class="language-plaintext highlighter-rouge">jupyter nbconvert --to html</code></a> to convert the notebook to an html page and then includes it in the post. Since <a href="https://jekyllrb.com/docs/configuration/markdown/">Kramdown</a> is the default Markdown renderer for Jekyll, we need to surround the call to the plugin with the <a href="https://kramdown.gettalong.org/syntax.html#extensions">::nomarkdown</a> tag so that it stops processing this part with Kramdown and outputs the content as-is.</p> <p>The plugin takes as input the path to the notebook, but it assumes the file exists. If you want to check if the file exists before calling the plugin, you can use the <code class="language-plaintext highlighter-rouge">file_exists</code> filter. This avoids getting a 404 error from the plugin and ending up displaying the main page inside of it instead. If the file does not exist, you can output a message to the user. The code displayed above outputs the following:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blog.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Note that the jupyter notebook supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="jupyter"/><summary type="html"><![CDATA[an example of a blog post with jupyter notebook]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://sachindots.github.io//blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with images</title><link href="https://sachindots.github.io//blog/2015/images/" rel="alternate" type="text/html" title="a post with images"/><published>2015-05-15T21:01:00+00:00</published><updated>2015-05-15T21:01:00+00:00</updated><id>https://sachindots.github.io//blog/2015/images</id><content type="html" xml:base="https://sachindots.github.io//blog/2015/images/"><![CDATA[<p>This is an example post with image galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9.jpg" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7.jpg" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8.jpg" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10.jpg" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/11.jpg" sizes="95vw"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12.jpg" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7.jpg" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">a post with formatting and links</title><link href="https://sachindots.github.io//blog/2015/formatting-and-links/" rel="alternate" type="text/html" title="a post with formatting and links"/><published>2015-03-15T16:40:16+00:00</published><updated>2015-03-15T16:40:16+00:00</updated><id>https://sachindots.github.io//blog/2015/formatting-and-links</id><content type="html" xml:base="https://sachindots.github.io//blog/2015/formatting-and-links/"><![CDATA[<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <h4 id="check-list">Check List</h4> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Brush Teeth</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on socks <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Put on left sock</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on right sock</li> </ul> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Go to school</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90’s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> <p>We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin</p> </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>